{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Example Code'\n",
    "subtitle: '6600 Final Project'\n",
    "author: Billy McGloin\n",
    "date: last-modified\n",
    "date-format: long\n",
    "format:\n",
    "  html:\n",
    "    self-contained: true\n",
    "    toc: true\n",
    "    code-overflow: wrap\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Conv2D, Flatten\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as KeyError\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape):\n",
    "        # Initialize the Replay Buffer with a maximum size and the shape of the input states\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0  # Counter to manage the next index to store a transition\n",
    "\n",
    "        # Allocate memory for each component of the environment's interaction\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float32)  # stores the states\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float32)  # stores the next states\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)  # stores the actions taken\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)  # stores the rewards received\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)  # stores the terminal status of the state (done flag)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        # Find the next available index in the buffer or overwrite if full\n",
    "        index = self.mem_cntr % self.mem_size  # Circular buffer\n",
    "        self.state_memory[index] = state  # Store the state\n",
    "        self.new_state_memory[index] = state_  # Store the next state after the action\n",
    "        self.action_memory[index] = action  # Store the action\n",
    "        self.reward_memory[index] = reward  # Store the reward\n",
    "        self.terminal_memory[index] = done  # Store the terminal state\n",
    "\n",
    "        self.mem_cntr += 1  # Increment the counter\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        # Sample a batch of transitions for training\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)  # Determine the size of the available memory\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)  # Randomly select indices\n",
    "\n",
    "        # Extract the sampled information from the buffer\n",
    "        states = self.state_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, new_states, dones  # Return the sampled batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "def build_dqn(lr, n_actions, input_dims, fcl_dims):\n",
    "    # This function constructs and returns a deep Q-network with specified parameters.\n",
    "    # lr: learning rate (not used in this function but typically used in compiling the model later)\n",
    "    # n_actions: the number of possible actions in the environment (size of the output layer)\n",
    "    # input_dims: dimensions of the state inputs from the environment\n",
    "    # fcl_dims: dimensions of the fully connected layers\n",
    "    \n",
    "    model = Sequential()  # Start with a sequential model\n",
    "    \n",
    "    # Add a convolutional layer with 32 filters, a kernel size of 8x8, a stride of 4, and ReLU activation.\n",
    "    # The input shape should be according to 'channels_first', meaning the channel dimension comes before the spatial dimensions.\n",
    "    model.add(Conv2D(filters=32, kernel_size=8, strides=4, activation='relu', input_shape=(*input_dims, 4), data_format='channels_first'))\n",
    "    \n",
    "    # Add a second convolutional layer with 64 filters, a kernel size of 4x4, a stride of 2, and ReLU activation.\n",
    "    model.add(Conv2D(filters=64, kernel_size=4, strides=2, activation='relu', data_format='channels_first'))\n",
    "    \n",
    "    # Add a third convolutional layer with 64 filters, a kernel size of 3x3, a stride of 1, and ReLU activation.\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu', data_format='channels_first'))\n",
    "    \n",
    "    # Flatten the output from the convolutional layers to feed it into fully connected layers.\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Add a fully connected layer with neurons equal to the first element of fcl_dims and ReLU activation.\n",
    "    model.add(Dense(fcl_dims[0], activation='relu'))\n",
    "    \n",
    "    # Add the output fully connected layer with n_actions neurons. This will output the Q-values for each action.\n",
    "    model.add(Dense(n_actions))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr = lr), loss='mean_squared_error')  # Compile the model with Adam optimizer and MSE loss\n",
    "\n",
    "    return model  # Return the constructed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size, replace, input_dims, eps_dec=1e-5, eps_min=0.01, mem_size=1000000, q_eval_fname='q_eval.h5', q_target_fname='q_target.h5'):\n",
    "        # Initializes the agent with given parameters and sets up neural networks for action evaluation and target estimation.\n",
    "        self.action_space = [i for i in range(n_actions)]  # List of all possible actions.\n",
    "        self.gamma = gamma  # Discount factor for future rewards.\n",
    "        self.epsilon = epsilon  # Initial probability for taking a random action.\n",
    "        self.eps_dec = eps_dec  # Rate at which to decrease epsilon.\n",
    "        self.eps_min = eps_min  # Minimum value for epsilon.\n",
    "        self.batch_size = batch_size  # Number of experiences to use in each learning step.\n",
    "        self.replace = replace  # Frequency at which target network weights are replaced with evaluation network weights.\n",
    "        self.q_target_model_file = q_target_fname  # Path to save the target model.\n",
    "        self.q_eval_model_file = q_eval_fname  # Path to save the evaluation model.\n",
    "        self.learn_step = 0  # Counter for steps of learning (used for updating the target network).\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims)  # Replay buffer for storing experience tuples.\n",
    "        self.q_eval = build_dqn(alpha, n_actions, input_dims, 512)  # Build the evaluation network.\n",
    "        self.q_next = build_dqn(alpha, n_actions, input_dims, 512)  # Build the target network.\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        # Replaces target network weights with those of the evaluation network if conditions are met.\n",
    "        if self.replace != 0 and self.learn_step % self.replace == 0:\n",
    "            self.q_next.set_weights(self.q_eval.get_weights())\n",
    "\n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        # Stores the transition in the replay buffer.\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # Chooses an action using an epsilon-greedy strategy.\n",
    "        if np.random.rand() < self.epsilon:  # Random action with probability epsilon.\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:  # Choose action with highest Q-value.\n",
    "            state = np.array([observation], copy=False, dtype=np.float32)\n",
    "            actions = self.q_eval.predict(state)\n",
    "            action = np.argmax(actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # Learns from a batch of experiences from the replay buffer if enough samples are available.\n",
    "        if self.memory.mem_cntr > self.batch_size:\n",
    "            state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "            self.replace_target_network()  # Updates the target network periodically.\n",
    "            q_eval = self.q_eval.predict(state)\n",
    "            q_next = self.q_next.predict(new_state)\n",
    "            q_next[done] = 0.0  # Zero out the values for terminal states.\n",
    "            indices = np.arange(self.batch_size)\n",
    "            q_target = q_eval[:]\n",
    "            q_target[indices, action] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "            self.q_eval.train_on_batch(state, q_target)\n",
    "            self.epsilon = max(self.epsilon - self.eps_dec, self.eps_min)  # Decrement epsilon but keep it above the minimum.\n",
    "            self.learn_step += 1  # Increment the learn step count.\n",
    "\n",
    "    def save_models(self):\n",
    "        # Saves the current state of the model weights.\n",
    "        self.q_eval.save(self.q_eval_model_file)\n",
    "        self.q_next.save(self.q_target_model_file)\n",
    "        print('... saving models ...')\n",
    "\n",
    "    def load_models(self):\n",
    "        # Loads the model weights from files.\n",
    "        self.q_eval = load_model(self.q_eval_model_file)\n",
    "        self.q_next = load_model(self.q_target_model_file)\n",
    "        print('... loading models ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLearning(x, scores, epsilons, filename, window):\n",
    "    # Create a figure object to hold the plots\n",
    "    fig = plt.figure()\n",
    "    # Add a subplot for the epsilon values with label \"1\"\n",
    "    ax = fig.add_subplot(111, label=\"1\")\n",
    "    # Add a second subplot for the scores with label \"2\", without interfering with the first\n",
    "    ax2 = fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "    \n",
    "    # Plot epsilon values on the first axis\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    # Set labels and colors for the first axis (epsilon)\n",
    "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    # Calculate a running average of the scores with specified window size\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(scores[max(0, t-window):(t+1)])\n",
    "\n",
    "    # Plot the running average of scores on the second axis\n",
    "    ax2.scatter(x, running_avg, color=\"C1\")\n",
    "    # Hide the x-axis details for the second subplot and move the y-axis to the right\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def plotLearningNoEpsilons(scores, filename, x=None, window=5):\n",
    "    # Determine the number of scores to compute the running average\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)  # Initialize an array to store the running average of scores\n",
    "    for t in range(N):\n",
    "        # Compute the running average using a sliding window approach\n",
    "        running_avg[t] = np.mean(scores[max(0, t-window):(t+1)])\n",
    "    \n",
    "    # If no x values are provided, generate a sequential list starting from 0 to N-1\n",
    "    if x is None:\n",
    "        x = [i for i in range(N)]\n",
    "    \n",
    "    # Set up labels for the axes\n",
    "    plt.ylabel('Score')  # Label for the y-axis\n",
    "    plt.xlabel('Game')   # Label for the x-axis\n",
    "    \n",
    "    # Plot the running average of scores against the game number or provided x values\n",
    "    plt.plot(x, running_avg)\n",
    "    \n",
    "    # Save the plot to a file specified by 'filename'\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# SkipEnv is a custom wrapper for the environment that repeats the same action a fixed number of times\n",
    "# and accumulates the rewards over those steps.\n",
    "class SkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(SkipEnv, self).__init__(env)  # Initialize the parent Wrapper class with the environment.\n",
    "        self._skip = skip  # Number of times to repeat action.\n",
    "\n",
    "    def step(self, action):\n",
    "        t_reward = 0.0  # Total reward is initialized to 0.\n",
    "        done = False  # Initialize 'done' to False.\n",
    "        # Repeat action 'skip' times and accumulate reward, unless the episode ends.\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            t_reward += reward\n",
    "            if done:\n",
    "                break  # If the episode is done, exit the loop.\n",
    "        return obs, t_reward, done, info  # Return the accumulated reward and last observation.\n",
    "\n",
    "# PreprocessFrame is a wrapper that preprocesses the observation from the environment\n",
    "# to a simplified format (grayscale and downscaled).\n",
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(PreprocessFrame, self).__init__(env)  # Initialize the parent ObservationWrapper class.\n",
    "        # Set observation space to 80x80 with 1 channel (grayscale).\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=255, shape=(80,80,1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # Use the static method 'process' on the observation.\n",
    "        return PreprocessFrame.process(obs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        # Convert frame to grayscale and downscale it for easier processing.\n",
    "        new_frame = np.reshape(frame, frame.shape).astype(np.float32)\n",
    "        new_frame = 0.299*new_frame[:,:,0] + 0.587*new_frame[:,:,1] + 0.114*new_frame[:,:,2]\n",
    "        # Downsample by taking only every second pixel.\n",
    "        new_frame = new_frame[35:195:2, ::2].reshape(80,80,1)\n",
    "        return new_frame.astype(np.uint8)\n",
    "\n",
    "# MoveImgChannel is a wrapper that changes the order of the channels in the observation space.\n",
    "class MoveImgChannel(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(MoveImgChannel, self).__init__(env)  # Initialize the parent class.\n",
    "        # Modify observation space to have channels as the first dimension.\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(self.observation_space.shape[-1], self.observation_space.shape[0], self.observation_space.shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # Move the last axis of the observation to the first position.\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "    \n",
    "# ScaleFrame is a wrapper that normalizes pixel values in the observation.\n",
    "class ScaleFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        # Scale the observation by dividing each pixel value by 255.\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "# BufferWrapper is a wrapper that stacks multiple observations to create a temporal buffer.\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps):\n",
    "        super(BufferWrapper, self).__init__(env)  # Initialize the parent class.\n",
    "        # Repeat the observation space for 'n_steps' to create a buffer.\n",
    "        self.observation_space = gym.spaces.Box(env.observation_space.low.repeat(n_steps, axis=0),\n",
    "                                                env.observation_space.high.repeat(n_steps, axis=0), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a buffer of zeros with the shape of the observation space.\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=np.float32)\n",
    "        # Initialize the buffer with the first observation.\n",
    "        return self.observation(self.env.reset())\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        # Update the buffer with new observation at the end, removing the oldest observation.\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "    \n",
    "# make_env is a utility function to create an environment with all the wrappers applied.\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)  # Create the original environment.\n",
    "    env = SkipEnv(env)  # Apply SkipEnv wrapper.\n",
    "    env = PreprocessFrame(env)  # Apply PreprocessFrame wrapper.\n",
    "    env = MoveImgChannel(env)  # Apply MoveImgChannel wrapper.\n",
    "    env = BufferWrapper(env, 4)  # Apply BufferWrapper wrapper with a buffer of 4 observations.\n",
    "    return ScaleFrame(env)  # Return the environment with all wrappers, including scaling the frame values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtown",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
