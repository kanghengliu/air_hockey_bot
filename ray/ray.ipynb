{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ray/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-20 15:34:40,337\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import ray \n",
    "import time\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 200  # The number of hidden layer neurons.\n",
    "gamma = 0.99  # The discount factor for reward.\n",
    "decay_rate = 0.99  # The decay factor for RMSProp leaky sum of grad^2.\n",
    "D = 80 * 80  # The input dimensionality: 80x80 grid.\n",
    "learning_rate = 1e-4  # Magnitude of the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # Crop the image.\n",
    "    img = img[35:195]\n",
    "    # Downsample by factor of 2.\n",
    "    img = img[::2, ::2, 0]\n",
    "    # Erase background (background type 1).\n",
    "    img[img == 144] = 0\n",
    "    # Erase background (background type 2).\n",
    "    img[img == 109] = 0\n",
    "    # Set everything else (paddles, ball) to 1.\n",
    "    img[img != 0] = 1\n",
    "    return img.astype(float).ravel()\n",
    "\n",
    "\n",
    "def process_rewards(r):\n",
    "    \"\"\"Compute discounted reward from a vector of rewards.\"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        # Reset the sum, since this was a game boundary (pong specific!).\n",
    "        if r[t] != 0:\n",
    "            running_add = 0\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "def rollout(model, env):\n",
    "    \"\"\"Evaluates  env and model until the env returns \"Terminated\" or \"Truncated\".\n",
    "\n",
    "    Returns:\n",
    "        xs: A list of observations\n",
    "        hs: A list of model hidden states per observation\n",
    "        dlogps: A list of gradients\n",
    "        drs: A list of rewards.\n",
    "\n",
    "    \"\"\"\n",
    "    # Reset the game.\n",
    "    observation, info = env.reset()\n",
    "    # Note that prev_x is used in computing the difference frame.\n",
    "    prev_x = None\n",
    "    xs, hs, dlogps, drs = [], [], [], []\n",
    "    terminated = truncated = False\n",
    "    while not terminated and not truncated:\n",
    "        cur_x = preprocess(observation)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "        prev_x = cur_x\n",
    "\n",
    "        aprob, h = model.policy_forward(x)\n",
    "        # Sample an action.\n",
    "        action = 2 if np.random.uniform() < aprob else 3\n",
    "\n",
    "        # The observation.\n",
    "        xs.append(x)\n",
    "        # The hidden state.\n",
    "        hs.append(h)\n",
    "        y = 1 if action == 2 else 0  # A \"fake label\".\n",
    "        # The gradient that encourages the action that was taken to be\n",
    "        # taken (see http://cs231n.github.io/neural-networks-2/#losses if\n",
    "        # confused).\n",
    "        dlogps.append(y - aprob)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Record reward (has to be done after we call step() to get reward\n",
    "        # for previous action).\n",
    "        drs.append(reward)\n",
    "    return xs, hs, dlogps, drs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"This class holds the neural network weights.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weights = {}\n",
    "        self.weights[\"W1\"] = np.random.randn(H, D) / np.sqrt(D)\n",
    "        self.weights[\"W2\"] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "    def policy_forward(self, x):\n",
    "        h = np.dot(self.weights[\"W1\"], x)\n",
    "        h[h < 0] = 0  # ReLU nonlinearity.\n",
    "        logp = np.dot(self.weights[\"W2\"], h)\n",
    "        # Softmax\n",
    "        p = 1.0 / (1.0 + np.exp(-logp))\n",
    "        # Return probability of taking action 2, and hidden state.\n",
    "        return p, h\n",
    "\n",
    "    def policy_backward(self, eph, epx, epdlogp):\n",
    "        \"\"\"Backward pass to calculate gradients.\n",
    "\n",
    "        Arguments:\n",
    "            eph: Array of intermediate hidden states.\n",
    "            epx: Array of experiences (observations).\n",
    "            epdlogp: Array of logps (output of last layer before softmax).\n",
    "\n",
    "        \"\"\"\n",
    "        dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "        dh = np.outer(epdlogp, self.weights[\"W2\"])\n",
    "        # Backprop relu.\n",
    "        dh[eph <= 0] = 0\n",
    "        dW1 = np.dot(dh.T, epx)\n",
    "        return {\"W1\": dW1, \"W2\": dW2}\n",
    "\n",
    "    def update(self, grad_buffer, rmsprop_cache, lr, decay):\n",
    "        \"\"\"Applies the gradients to the model parameters with RMSProp.\"\"\"\n",
    "        for k, v in self.weights.items():\n",
    "            g = grad_buffer[k]\n",
    "            rmsprop_cache[k] = decay * rmsprop_cache[k] + (1 - decay) * g ** 2\n",
    "            self.weights[k] += lr * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "\n",
    "\n",
    "def zero_grads(grad_buffer):\n",
    "    \"\"\"Reset the batch gradient buffer.\"\"\"\n",
    "    for k, v in grad_buffer.items():\n",
    "        grad_buffer[k] = np.zeros_like(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 15:34:42,000\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# This forces OpenMP to use 1 single thread, which is needed to \n",
    "# prevent contention between multiple actors. \n",
    "# See https://docs.ray.io/en/latest/ray-core/configure.html for \n",
    "# more details. \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# Tell numpy to only use one core. If we don't do this, each actor may\n",
    "# try to use all of the cores and the resulting contention may result\n",
    "# in no speedup over the serial version. Note that if numpy is using\n",
    "# OpenBLAS, then you need to set OPENBLAS_NUM_THREADS=1, and you\n",
    "# probably need to do it from the command line (so it happens before\n",
    "# numpy is imported).\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "ray.init()\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class RolloutWorker(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"ALE/Pong-v5\")\n",
    "\n",
    "    def compute_gradient(self, model):\n",
    "        # Compute a simulation episode.\n",
    "        xs, hs, dlogps, drs = rollout(model, self.env)\n",
    "        reward_sum = sum(drs)\n",
    "        # Vectorize the arrays.\n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "\n",
    "        # Compute the discounted reward backward through time.\n",
    "        discounted_epr = process_rewards(epr)\n",
    "        # Standardize the rewards to be unit normal (helps control the gradient\n",
    "        # estimator variance).\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "        # Modulate the gradient with advantage (the policy gradient magic\n",
    "        # happens right here).\n",
    "        epdlogp *= discounted_epr\n",
    "        return model.policy_backward(eph, epx, epdlogp), reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=98902)\u001b[0m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "\u001b[36m(RolloutWorker pid=98902)\u001b[0m [Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 computed 4 rollouts in 1.150871992111206 seconds, running mean is -20.990199\n",
      "Batch 2 computed 4 rollouts in 0.6837079524993896 seconds, running mean is -20.990585198505993\n",
      "Batch 3 computed 4 rollouts in 0.822894811630249 seconds, running mean is -20.96095617924992\n",
      "Batch 4 computed 4 rollouts in 0.5655901432037354 seconds, running mean is -20.95249466157232\n",
      "Batch 5 computed 4 rollouts in 0.7737348079681396 seconds, running mean is -20.934366561452673\n",
      "Batch 6 computed 4 rollouts in 0.780846118927002 seconds, running mean is -20.89705278080886\n",
      "Batch 7 computed 4 rollouts in 0.7948031425476074 seconds, running mean is -20.871109312004396\n",
      "Batch 8 computed 4 rollouts in 0.5742099285125732 seconds, running mean is -20.86618811938527\n",
      "Batch 9 computed 4 rollouts in 0.8168528079986572 seconds, running mean is -20.841759841390896\n",
      "Batch 10 computed 4 rollouts in 0.7197949886322021 seconds, running mean is -20.808294135018325\n",
      "Batch 11 computed 4 rollouts in 0.6590211391448975 seconds, running mean is -20.776245121005\n",
      "Batch 12 computed 4 rollouts in 0.6812610626220703 seconds, running mean is -20.77506195601937\n",
      "Batch 13 computed 4 rollouts in 0.7455871105194092 seconds, running mean is -20.754222422455005\n",
      "Batch 14 computed 4 rollouts in 0.7498412132263184 seconds, running mean is -20.74410603966281\n",
      "Batch 15 computed 4 rollouts in 0.9006860256195068 seconds, running mean is -20.724488282716997\n",
      "Batch 16 computed 4 rollouts in 0.6637356281280518 seconds, running mean is -20.705742543669704\n",
      "Batch 17 computed 4 rollouts in 0.6624798774719238 seconds, running mean is -20.70733746153637\n",
      "Batch 18 computed 4 rollouts in 0.7583990097045898 seconds, running mean is -20.678969533275367\n",
      "Batch 19 computed 4 rollouts in 0.6450037956237793 seconds, running mean is -20.671619414575883\n",
      "Batch 20 computed 4 rollouts in 0.8024449348449707 seconds, running mean is -20.64465891988013\n"
     ]
    }
   ],
   "source": [
    "iterations = 20\n",
    "batch_size = 4\n",
    "model = Model()\n",
    "actors = [RolloutWorker.remote() for _ in range(batch_size)]\n",
    "\n",
    "running_reward = None\n",
    "# \"Xavier\" initialization.\n",
    "# Update buffers that add up gradients over a batch.\n",
    "grad_buffer = {k: np.zeros_like(v) for k, v in model.weights.items()}\n",
    "# Update the rmsprop memory.\n",
    "rmsprop_cache = {k: np.zeros_like(v) for k, v in model.weights.items()}\n",
    "\n",
    "for i in range(1, 1 + iterations):\n",
    "    model_id = ray.put(model)\n",
    "    gradient_ids = []\n",
    "    # Launch tasks to compute gradients from multiple rollouts in parallel.\n",
    "    start_time = time.time()\n",
    "    gradient_ids = [actor.compute_gradient.remote(model_id) for actor in actors]\n",
    "    for batch in range(batch_size):\n",
    "        [grad_id], gradient_ids = ray.wait(gradient_ids)\n",
    "        grad, reward_sum = ray.get(grad_id)\n",
    "        # Accumulate the gradient over batch.\n",
    "        for k in model.weights:\n",
    "            grad_buffer[k] += grad[k]\n",
    "        running_reward = (\n",
    "            reward_sum\n",
    "            if running_reward is None\n",
    "            else running_reward * 0.99 + reward_sum * 0.01\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    print(\n",
    "        \"Batch {} computed {} rollouts in {} seconds, \"\n",
    "        \"running mean is {}\".format(\n",
    "            i, batch_size, end_time - start_time, running_reward\n",
    "        )\n",
    "    )\n",
    "    model.update(grad_buffer, rmsprop_cache, learning_rate, decay_rate)\n",
    "    zero_grads(grad_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
