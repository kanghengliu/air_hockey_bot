{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 13:39:25.857355: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-20 13:39:25.859405: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-20 13:39:25.884183: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 13:39:26.377414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# coee based on https://wingedsheep.com/lunar-lander-dqn/\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import uuid\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (8,)\n",
      "Output: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "print(f\"Input: {env.observation_space.shape}\")\n",
    "print(f\"Output: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_huber_loss(mask_value, clip_delta):\n",
    "  def f(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    cond  = K.abs(error) < clip_delta\n",
    "    mask_true = K.cast(K.not_equal(y_true, mask_value), K.floatx())\n",
    "    masked_squared_error = 0.5 * K.square(mask_true * (y_true - y_pred))\n",
    "    linear_loss  = mask_true * (clip_delta * K.abs(error) - 0.5 * (clip_delta ** 2))\n",
    "    huber_loss = tf.where(cond, masked_squared_error, linear_loss)\n",
    "    return K.sum(huber_loss) / K.sum(mask_true)\n",
    "  f.__name__ = 'masked_huber_loss'\n",
    "  return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (9,) # 8 variables in the environment + the fraction finished we add ourselves\n",
    "outputs = 4\n",
    "\n",
    "def create_model(learning_rate, regularization_factor):\n",
    "  model = Sequential([\n",
    "    Dense(64, input_shape=input_shape, activation=\"relu\", kernel_regularizer=l2(regularization_factor)),\n",
    "    Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor)),\n",
    "    Dense(64, activation=\"relu\", kernel_regularizer=l2(regularization_factor)),\n",
    "    Dense(outputs, activation='linear', kernel_regularizer=l2(regularization_factor))\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "  model.compile(optimizer=optimizer, loss=masked_huber_loss(0.0, 1.0))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_epsilon_greedy(q_values, epsilon):\n",
    "  random_value = random.uniform(0, 1)\n",
    "  if random_value < epsilon: \n",
    "    return random.randint(0, len(q_values) - 1)\n",
    "  else:\n",
    "    return np.argmax(q_values)\n",
    "\n",
    "def select_best_action(q_values):\n",
    "  return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateTransition():\n",
    "\n",
    "  def __init__(self, old_state, action, reward, new_state, done):\n",
    "    self.old_state = old_state\n",
    "    self.action = action\n",
    "    self.reward = reward\n",
    "    self.new_state = new_state\n",
    "    self.done = done\n",
    "\n",
    "class ReplayBuffer():\n",
    "  current_index = 0\n",
    "\n",
    "  def __init__(self, size = 10000):\n",
    "    self.size = size\n",
    "    self.transitions = []\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.transitions) < self.size: \n",
    "      self.transitions.append(transition)\n",
    "    else:\n",
    "      self.transitions[self.current_index] = transition\n",
    "      self.__increment_current_index()\n",
    "\n",
    "  def length(self):\n",
    "    return len(self.transitions)\n",
    "\n",
    "  def get_batch(self, batch_size):\n",
    "    return random.sample(self.transitions, batch_size)\n",
    "\n",
    "  def __increment_current_index(self):\n",
    "    self.current_index += 1\n",
    "    if self.current_index >= self.size - 1: \n",
    "      self.current_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_target_values(model, target_model, state_transitions, discount_factor):\n",
    "  states = []\n",
    "  new_states = []\n",
    "  for transition in state_transitions:\n",
    "    states.append(transition.old_state)\n",
    "    new_states.append(transition.new_state)\n",
    "\n",
    "  new_states = np.array(new_states)\n",
    "\n",
    "  q_values_new_state = get_multiple_q_values(model, new_states)\n",
    "  q_values_new_state_target_model = get_multiple_q_values(target_model, new_states)\n",
    "  \n",
    "  targets = []\n",
    "  for index, state_transition in enumerate(state_transitions):\n",
    "    best_action = select_best_action(q_values_new_state[index])\n",
    "    best_action_next_state_q_value = q_values_new_state_target_model[index][best_action]\n",
    "    \n",
    "    if state_transition.done:\n",
    "      target_value = state_transition.reward\n",
    "    else:\n",
    "      target_value = state_transition.reward + discount_factor * best_action_next_state_q_value\n",
    "\n",
    "    target_vector = [0] * outputs\n",
    "    target_vector[state_transition.action] = target_value\n",
    "    targets.append(target_vector)\n",
    "\n",
    "  return np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, states, targets):\n",
    "  model.fit(states, targets, epochs=1, batch_size=len(targets), verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model(model):\n",
    "  now = datetime.now()\n",
    "  backup_file = now.strftime(\"backup_%Y%m%d-%H-%M-%S.keras\")\n",
    "  model.save(backup_file)\n",
    "  new_model = load_model(backup_file, custom_objects={ 'masked_huber_loss': masked_huber_loss(0.0, 1.0) })\n",
    "  # shutil.rmtree(backup_file)\n",
    "  os.remove(backup_file)\n",
    "  return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageRewardTracker():\n",
    "  current_index = 0\n",
    "\n",
    "  def __init__(self, num_rewards_for_average=100):\n",
    "    self.num_rewards_for_average = num_rewards_for_average\n",
    "    self.last_x_rewards = []\n",
    "\n",
    "  def add(self, reward):\n",
    "    if len(self.last_x_rewards) < self.num_rewards_for_average: \n",
    "      self.last_x_rewards.append(reward)\n",
    "    else:\n",
    "      self.last_x_rewards[self.current_index] = reward\n",
    "      self.__increment_current_index()\n",
    "\n",
    "  def __increment_current_index(self):\n",
    "    self.current_index += 1\n",
    "    if self.current_index >= self.num_rewards_for_average: \n",
    "      self.current_index = 0\n",
    "\n",
    "  def get_average(self):\n",
    "    return np.average(self.last_x_rewards)\n",
    "\n",
    "\n",
    "class FileLogger():\n",
    "\n",
    "  def __init__(self, file_name='progress.log'):\n",
    "    self.file_name = file_name\n",
    "    self.clean_progress_file()\n",
    "\n",
    "  def log(self, episode, steps, reward, average_reward):\n",
    "    f = open(self.file_name, 'a+')\n",
    "    f.write(f\"{episode};{steps};{reward};{average_reward}\\n\")\n",
    "    f.close()\n",
    "\n",
    "  def clean_progress_file(self):\n",
    "    if os.path.exists(self.file_name):\n",
    "      os.remove(self.file_name)\n",
    "    f = open(self.file_name, 'a+')\n",
    "    f.write(\"episode;steps;reward;average\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_size = 200000\n",
    "learning_rate = 0.001\n",
    "regularization_factor = 0.001\n",
    "training_batch_size = 128\n",
    "training_start = 256\n",
    "max_episodes = 10\n",
    "max_steps = 1000\n",
    "target_network_replace_frequency_steps = 1000\n",
    "model_backup_frequency_episodes = 100\n",
    "starting_epsilon = 1.0\n",
    "minimum_epsilon = 0.01\n",
    "epsilon_decay_factor_per_episode = 0.995\n",
    "discount_factor = 0.99\n",
    "train_every_x_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 0 with epsilon 1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     14\u001b[0m fraction_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 15\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfraction_finished\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m first_q_values \u001b[38;5;241m=\u001b[39m get_q_values(model, state)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_q_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/DSAN/lib/python3.11/site-packages/numpy/lib/function_base.py:5493\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_append_dispatcher)\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(arr, values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5446\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5447\u001b[0m \u001b[38;5;124;03m    Append values to the end of an array.\u001b[39;00m\n\u001b[1;32m   5448\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5491\u001b[0m \n\u001b[1;32m   5492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5493\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5494\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5495\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "model = create_model(learning_rate, regularization_factor)\n",
    "target_model = copy_model(model)\n",
    "epsilon = starting_epsilon\n",
    "step_count = 0\n",
    "average_reward_tracker = AverageRewardTracker(100)\n",
    "file_logger = FileLogger()\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "  print(f\"Starting episode {episode} with epsilon {epsilon}\")\n",
    "\n",
    "  episode_reward = 0\n",
    "  state = env.reset()\n",
    "  fraction_finished = 0.0\n",
    "  state = np.append(state, fraction_finished)\n",
    "\n",
    "  first_q_values = get_q_values(model, state)\n",
    "  print(f\"Q values: {first_q_values}\")\n",
    "  print(f\"Max Q: {max(first_q_values)}\")\n",
    "\n",
    "  for step in range(1, max_steps + 1):\n",
    "    step_count += 1\n",
    "    q_values = get_q_values(model, state)\n",
    "    action = select_action_epsilon_greedy(q_values, epsilon)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    fraction_finished = (step + 1) / max_steps\n",
    "    new_state = np.append(new_state, fraction_finished)\n",
    "    \n",
    "    episode_reward += reward\n",
    "\n",
    "    if step == max_steps:\n",
    "      print(f\"Episode reached the maximum number of steps. {max_steps}\")\n",
    "      done = True\n",
    "\n",
    "    state_transition = StateTransition(state, action, reward, new_state, done)\n",
    "    replay_buffer.add(state_transition)\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "    if step_count % target_network_replace_frequency_steps == 0:\n",
    "      print(\"Updating target model\")\n",
    "      target_model = copy_model(model)\n",
    "\n",
    "    if replay_buffer.length() >= training_start and step_count % train_every_x_steps == 0:\n",
    "      batch = replay_buffer.get_batch(batch_size=training_batch_size)\n",
    "      targets = calculate_target_values(model, target_model, batch, discount_factor)\n",
    "      states = np.array([state_transition.old_state for state_transition in batch])\n",
    "      train_model(model, states, targets)\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  average_reward_tracker.add(episode_reward)\n",
    "  average = average_reward_tracker.get_average()\n",
    "\n",
    "  print(\n",
    "    f\"episode {episode} finished in {step} steps with reward {episode_reward}. \"\n",
    "    f\"Average reward over last 100: {average}\")\n",
    "\n",
    "  if episode != 0 and episode % model_backup_frequency_episodes == 0:\n",
    "    backup_file = str(episode)+\".h5\"\n",
    "    print(f\"Backing up model to {backup_file}\")\n",
    "    model.save(backup_file)\n",
    "\n",
    "  epsilon *= epsilon_decay_factor_per_episode\n",
    "  epsilon = max(minimum_epsilon, epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
