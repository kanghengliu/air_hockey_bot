---
title: "Robo Rink: Reinforcement Learning Implementation on Custom Air Hockey Game"
subtitle: "DSAN 6600: Neural Nets and Deep Learning"
authors: ["Jorge Bris Moreno", "Eric Dapkus", "Brian Kwon", "Kangheng Liu", "Billy McGloin"]
date: last-modified
date-format: long
format:
  html:
    self-contained: true
    toc: true
    code-overflow: wrap
    code-fold: true
---

# Introduction (Brian)

Reinforcement learning is a third paradigm of a machine learning along with supervised and unsupervised learning. In reinforcement learning, an agent is trained to maximize the long-term rewards. This agent interacts with an environment by taking an action based on the policy, receives a reward, and updates its policy. The agent continues this cycle to maximize the rewards until it reaches a goal. This project is using RL to train a model to learn how to play our custom air hockey game, and eventually we can play against. The model, or an agent specifically in RL, are trained through Deep Q-learning. 

# Creating The Game (Brian)

We used Pygame to create our custom air hockey game. The `puck.py` and `paddle.py` files are the backbones of the game. They handle the basics of the game machanics, such as initializing puck and paddles, movement, and drawing them on Pygame screen. These two files are fed into `game.py` file. The `game.py` file controls the game logic, such as scoring and resetting the game when needed. The important part of the game is that it takes differents modes. It can take "pvp" (player v. player) if you want to play with somebody else, "pve" (player v. environment) when you are playing against the bot, and "rlve" which is reinforcement learning model versus environment. For the environment, we created two different simple bots to play and train against. One moves only up and down and the other only chases the ball. These two bots are used to train our RL agent. The `main.py` file wraps `game.py` with Pygame so that we can play our custom game using Pygame. Then we have `air_hock_env.py` and `dqn.py` files. Creating environment is a crucial part of our project as we are using Gynasium to train our RL model. The `air_hock_env.py` file connects our game with Gymnasium by creating custom environment wrapping the 4 main methods (reset, step, render, close). Using this custom envrionment, we train our RL agent in `dqn.py` file using Deep Q-Networks. 

# Wrapping Game in Gym Environment (Kang)

- gym is a toolkit for developing and comparing reinforcement learning algorithms
- what is the environment structure
- how is this then used for a RL algorithm

# Model Selection (Jorge)

### DQN
The choice of models for a project like this can vary. However, due to the simple game model generated, we will be using the DQN algorithm as we believe is a sufficiently strong neural network class for our case and it will allow us to experiment with different architectures. The DQN class is a form of Reinforcement Learning that utilizes neural networks to approximate the Q-function which traditionally is a table that holds Q-values for every state and action pairs. These values represent the quality of taking a specific step, and normally the step taking will be determined by the best value.

### Convolutional Architecture
Due to the screen size and the main objective of the game (move the paddle to hit the puck and score a goal), we first decide to experiment with a convolutional neural network that would take 3 frames at a time. The intended goal was for it to identify the location of paddles and the puck and identify the speed and angles from those three frames. The frames fed into the model where in grayscale (as color does not matter for this game) and it had the following architecture:

- First Convolutional Layer: This layer has 16 filters (or neurons), a kernel size of 3, a stride of 1, and padding of 1. It applies a convolution operation to the input data.
- First Batch Normalization Layer (bn1): After the first convolutional layer, there's a batch normalization layer with 16 features. The batch normalization is used to stabilize the learning process and dramatically reduce the number of training epochs required.
- First Pooling Layer: This is an average pooling layer with a window of 2, which reduces the spatial dimensions (height and width) of the output from the previous batch normalization layer.
- Second Convolutional Layer: This layer doubles the number of filters to 32, with the same kernel size, stride, and padding as the first convolutional layer.
- Second Batch Normalization Layer: This layer normalizes the output of the second convolutional layer, also with 32 features.
- Second Pooling Layer: Another average pooling layer with a window of 2 follows, further reducing the dimensions of the data.
- Flattening: Before passing the data to the fully connected layer, the multi-dimensional output from the pooling layers must be flattened into a single vector.
- Fully Connected Layer: This is the output layer. It maps the flattened features from the convolutional layers to the final output. The number of outputs corresponds to the number of possible actions (9) in the environment the DQN is interacting with, providing the Q-values for each action.

The activation functions utilized were ReLU and the possible actions are the following: 0: Do nothing, 1: Move up, 2: Move down, 3: Move left, 4: Move right, 5: Move up and left, 6: Move up and right, 7: Move down and left, 8: Move down and right.

After training this model, we realized that the model was taking too long to “train” and that the performance was not optimal. This could be due to the hyperparameters chosen such as the number of frames fed at a time, or other parameters in the architecture. Due to an effort find a better model, we decided to change our strategy as you can see below.

### Linear Architecture:
This second model tried to speed the training process and obtain a better performance of the Reinforcement Learning. To do so, we decided to train a Linear Neural Network that will take the information necessary for the algorithm to take the best actions. The model takes 8 inputs, x and y coordinates for the positioning of both paddles and puck (6 inputs), the velocity of the puck, and the direction of the puck. This would give the model the necessary information to make the right movements. We experimented with a simpler and a deeper model. The simpler one had two hidden layers and the deeper one had 3. The architecture of the deeper model is the following:

- First Hidden Layer (linear): Fully connected (dense) layer with 16 neurons.
- Second Hidden Layer (linear): Fully connected layer follows with 32 neurons.
- Third Hidden Layer (linear): This layer has 16 neurons.
- Output Layer (head): The number of outputs corresponds to the number of possible actions (9) in the environment the DQN is interacting with, providing the Q-values for each action.

The activation functions utilized were ReLU and the possible actions are the same as the previous model.

This architecture clearly outperformed the previous one. It does not only perform better but trains much faster. This is probably due to the specific selection of inputs given and the simple structure, which is able to receive and process all the necessary information efficiently. However, we decided to try one more model to see if we can obtain even a better algorithm using a different architecture.

### Convolutional & Linear Architecture
This architecture tried to combine convolutional layers to feed data into linear layers. It is a common practice as it is able to process frames, obtain all the relevant information from them, and learn any information from that information throughout the linear layers. However, due to our computational and time limitations, we had to simplify this architecture to the following:

- First Convolutional Layer: This layer has 16 filters with a kernel size of 4 and a stride of 1. It is followed by batch normalization.
- Second Convolutional Layer: Follows the first convolutional layer, with 32 filters, a kernel size of 4, and a stride of 1, also followed by batch normalization.
- Flattening: Before passing the data to the fully connected layer, the multi-dimensional output from the convolutional layers must be flattened into a single vector.
- Linear Layer: This is a linear layer that has an input dimension equal to the flattened output of the convolutional layers and an output dimension of 18.
- Output Layer: This is the final linear layer that maps the 18 features from the previous linear layer to the outputs dimension (9).

Again, we used ReLU as the activation function and the same possible actions as the previous models.

While we would have liked to make this architecture more complex and, for that reason, seems to be underperforming, this is something that can be explored in the future.

### Future model exploration
In future exploration, different hyperparameters, reward composition, or architectures could be explored in order to train a better model. Furthermore, while DQN seems to work well for our purpose, some other algorithms may be explored in order to compare different behaviors and performances among the models.


# Proof of Concept (Kang)

- created simplified air hockey game - kinda a mesh between air hockey and pong
- did this directly in a custom gym environment
- wanted to first test the environment and then train to demonstarte proof of concept
- short videos to display results

# Training Specs (Eric)

- Google Colab Pro, Intel NUC, Ubunto, etc.

# Training Process (Billy)

- Replay memory
- policy net
- target net
- selecting the action
- optimize functoin
- loss function

# Training Results (Eric & Billy)

- how models compared against each other (eric)
- maybe a plot or two with some descriptions (eric)
- how did the model perform (eric)
- include videos at various iterations (billy)

# Human v. RL - billy if accomplished

- this is now a feature we have. billy or eric will train a model to make it super good and then hopefully we will be able to host this
- if we can, maybe this part just includes a link

# Conclusion (Jorge)

This project presented challenges. Having incorporated mechanics and certain “realistic” features such as change of angles and speeds of the puck due to the nature of the shape and movement of the paddles, we had to take into account more variables than we previously did at the beginning. However, we have found models that really approximate how a “human” would actually play the game, making the playing against the train model very realistic. This was our original goal and we strongly believe that we have succeeded.

On the other hand, in order to improve our bot and make it a better performer, further exploration into different network classes, architectures, and other hyperparameters should be performed. While we were limited due to our computational abilities, after learning how the model works, different architectures impact the bot capabilities, and how different hyperparameter selection affects the training, we could explore different selections in a more targeted format. This is something that we may explore further in the future.